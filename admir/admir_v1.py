# -*- coding: utf-8 -*-
"""Copy of ADMIR_v1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1dgFPYHreLItCIQQgH8wwkpObZY6dT6SF

# Preamble
"""

!pip install --upgrade nibabel
#!pip install --upgrade numpy
!pip install voxelmorph

from google.colab import drive
drive.mount('/content/drive')

import numpy as np
from matplotlib import pyplot as plt
import neurite as ne
import nibabel as nb
import os, sys, glob
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.utils.data as Data
import torch.optim as optim
import torchvision
from torchvision import datasets, transforms
import neurite as ne
import time

print("nibabel version: {}".format(nb.__version__))
print("pytorch version: {}".format(torch.__version__))
print("numpy version: {}".format(np.__version__))

# data_path_t1 = "/content/drive/My Drive/Colab Notebooks/IR_DL/Batch_1/T1_F_T2_M/T1"
# data_path_t2 = "/content/drive/My Drive/Colab Notebooks/IR_DL/Batch_1/T1_F_T2_M/T2_affine"

# data_path_t1 = "/content/drive/MyDrive/DATASET_FINAL/Train/intramodal/pairwise_registered/T1/Batch_1/"
# data_path_t2 = "/content/drive/MyDrive/DATASET_FINAL/Train/intramodal/pairwise_registered/T1/Batch_1_F_Batch_2_M/"

data_path_t1 = "/content/drive/MyDrive/Colab Notebooks/IR_DL/Batch_1/pairwise_registered/T1/Batch_1/"
data_path_t2 = "/content/drive/MyDrive/DATASET_FINAL/Train/intramodal/pairwise_registered/T1/Batch_1_F_Batch_2_M/"

# data_path_t1 = "/content/drive/My Drive/Colab Notebooks/ImageRegistrationUsingDeepLearning/ICNet/Dataset/T1_resampled/"
# data_path_t2 = "/content/drive/My Drive/Colab Notebooks/ImageRegistrationUsingDeepLearning/ICNet/Dataset/T2_resampled/"
file_names_t1 = sorted(glob.glob(os.path.join(data_path_t1, "*.*")))
file_names_t2 = sorted(glob.glob(os.path.join(data_path_t2, "*.nii.gz")))

print(file_names_t1[0:5] + file_names_t1[45:])
print(file_names_t2[0:5] + file_names_t2[45:])
print(len(file_names_t1))
print(len(file_names_t2))

!ls -ltr

"""# Image Processing"""

def load_4D(name):
    model_np = np.zeros(shape=(128, 128, 128))
    X_nb = nb.load(name)
    X_np = X_nb.dataobj
    #print("Oreintation: {}".format(nb.aff2axcodes(X_nb.affine)))
    #model_np[:, :, 0:X_np.shape[2]] = X_np[0:128, 0:128, :]
    #model_np = np.reshape(model_np, (1,)+ model_np.shape)
    model_np = np.reshape(X_np, (1,)+ X_np.shape)
    return model_np

def imgnorm(N_I,index1=0.0001,index2=0.0001):
    I_sort = np.sort(N_I.flatten())
    I_min = I_sort[int(index1*len(I_sort))]
    I_max = I_sort[-int(index2*len(I_sort))]
    N_I =1.0*(N_I-I_min)/(I_max-I_min)
    N_I[N_I>1.0]=1.0
    N_I[N_I<0.0]=0.0
    N_I2 = N_I.astype(np.float32)
    return N_I2

def Norm_Zscore(img):
    img= (img-np.mean(img))/np.std(img) 
    return img

class Dataset(Data.Dataset):
  'Characterizes a dataset for PyTorch'
  def __init__(self, t1_filenames, t2_filenames, iterations=1,norm=True):
        'Initialization'
        self.t1_filenames = t1_filenames
        self.t2_filenames = t2_filenames
        self.norm = norm
        self.iterations = iterations
  def __len__(self):
        'Denotes the total number of samples'
        return len(self.t1_filenames) * self.iterations

  def __getitem__(self, idx):
        'Generates one sample of data'
        img_A = load_4D(self.t1_filenames[idx])
        img_B = load_4D(self.t2_filenames[idx])     
        
        if self.norm:
            #return  Norm_Zscore(imgnorm(img_A)) , Norm_Zscore(imgnorm(img_B))
            return  imgnorm(img_A) , imgnorm(img_B)
        else:
            return img_A, img_B

training_generator = Data.DataLoader(Dataset(file_names_t1, file_names_t2,norm=True), batch_size=4, shuffle=False)

validation_generator = Data.DataLoader(Dataset(file_names_t1, file_names_t2,norm=True), batch_size=1, shuffle=False)

counter = 0
for fp in file_names_t1:
  myimg = load_4D(fp)
  myimg_norm = imgnorm(myimg)
  myimg_znorm = Norm_Zscore(myimg_norm)
  print(fp)
  print("Max values of natural image, normalized and z-normalized are: {}, {} and {}".format(np.max(myimg), np.max(myimg_norm), np.max(myimg_znorm) ))
  print("Min values of natural image, normalized and z-normalized are: {}, {} and {}".format(np.min(myimg), np.min(myimg_norm), np.min(myimg_znorm) ))
  print("Mean values of natural image, normalized and z-normalized are: {}, {} and {}".format(np.mean(myimg), np.mean(myimg_norm), np.mean(myimg_znorm) ))
  print("========= ========== ======")
  print()
  counter = counter + 1
  if (counter > 2):
    break;


#print(np.max(myimg))
#print(np.min(myimg1))
#print(np.min(myimg2))

for X,Y in training_generator:
  print(X.shape)
  print(Y.shape)
  break

"""# Spatial Transform"""

class SpatialTransformer(nn.Module):
    """
    N-D Spatial Transformer
    """

    def __init__(self, size, is_affine=False, theta = None, mode='bilinear', affine_image_size =  (2, 1, 128, 128, 128)):
        super().__init__()

        self.mode = mode
        self.isaffine = is_affine
        self.theta = theta
        self.affine_image_size =  affine_image_size
        # create sampling grid
        # registering the grid as a buffer cleanly moves it to the GPU, but it also
        # adds it to the state dict. this is annoying since everything in the state dict
        # is included when saving weights to disk, so the model files are way bigger
        # than they need to be. so far, there does not appear to be an elegant solution.
        # see: https://discuss.pytorch.org/t/how-to-register-buffer-without-polluting-state-dict

        if (self.isaffine):
          grid = F.affine_grid(self.theta, self.affine_image_size, align_corners=False)
          #grid = grid.permute(0, 4, 1, 2, 3)
          self.register_buffer('grid', grid)
        else:
          vectors = [torch.arange(0, s) for s in size]
          grids = torch.meshgrid(vectors)
          grid = torch.stack(grids)
          grid = torch.unsqueeze(grid, 0)
          grid = grid.type(torch.FloatTensor)
          self.register_buffer('grid', grid)

    def forward(self, src, flow=None):      
      if (self.isaffine):
        grid = F.affine_grid(self.theta, self.affine_image_size)        
        warped_image = F.grid_sample(src, grid)
        #warped_image = warped_image.permute(0, 4, 1, 2, 3)
        return warped_image
      else:
        # new locations
        new_locs = self.grid + flow
        shape = flow.shape[2:]

        # need to normalize grid values to [-1, 1] for resampler
        for i in range(len(shape)):
            new_locs[:, i, ...] = 2 * (new_locs[:, i, ...] / (shape[i] - 1) - 0.5)

        # move channels dim to last position
        # also not sure why, but the channels need to be reversed
        if len(shape) == 2:
            new_locs = new_locs.permute(0, 2, 3, 1)
            new_locs = new_locs[..., [1, 0]]
        elif len(shape) == 3:
            new_locs = new_locs.permute(0, 2, 3, 4, 1)
            new_locs = new_locs[..., [2, 1, 0]]

        return F.grid_sample(src, new_locs, align_corners=True, mode=self.mode)

"""# Deformable ConvNet"""

class Admir_Deformable_UNet(nn.Module):
  def __init__(self,in_channel  , n_classes,start_channel):
        self.in_channel = in_channel
        self.n_classes = n_classes
        self.start_channel = start_channel
        super(Admir_Deformable_UNet, self).__init__()
        self.eninput = self.encoder(self.in_channel, self.start_channel, bias=False)

        self.ec1 = self.encoder(self.start_channel, self.start_channel, bias=False)
        self.ec2 = self.encoder(self.start_channel, self.start_channel*2, stride=2, bias=False)

        self.ec3 = self.encoder(self.start_channel*2, self.start_channel*2, bias=False)
        self.ec4 = self.encoder(self.start_channel*2, self.start_channel*4, stride=2, bias=False)

        self.ec5 = self.encoder(self.start_channel*4, self.start_channel*4, bias=False)
        self.ec6 = self.encoder(self.start_channel*4, self.start_channel*8, stride=2, bias=False)

       
    
        self.dc1 = self.encoder(self.start_channel*8, self.start_channel*8, kernel_size=3, stride=1, bias=False) 
        self.dc2 = self.encoder(self.start_channel*4, self.start_channel*4, kernel_size=3, stride=1, bias=False)          
        self.dc3 = self.encoder(self.start_channel*2, self.start_channel*2, kernel_size=3, stride=1, bias=False)

        self.up1 = self.decoder(self.start_channel*8, self.start_channel*4)
        self.up2 = self.decoder(self.start_channel*4, self.start_channel*2)
        self.up3 = self.decoder(self.start_channel*2, self.start_channel)

        self.dc4 = self.output(self.start_channel, self.n_classes,kernel_size=1,bias=False)

  def encoder(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1,
                bias=True):
    layer = nn.Sequential(
                nn.Conv3d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, bias=bias),
                nn.BatchNorm3d(out_channels),
                nn.LeakyReLU())
    return layer

  def decoder(self, in_channels, out_channels, kernel_size=2, stride=2, padding=0,
                output_padding=0, bias=True):
    layer = nn.Sequential(
                nn.ConvTranspose3d(in_channels, out_channels, kernel_size, stride=stride,
                               padding=padding, output_padding=output_padding, bias=bias),
                                nn.LeakyReLU())
    return layer
       
  def output(self, in_channels, out_channels, kernel_size=3, 
                bias=False, batchnorm=False):
    layer = nn.Sequential(
                nn.Conv3d(in_channels, out_channels, kernel_size, bias=bias),
               )
    return layer

  def forward(self, x,y):
        # print("x,y", x.shape, "  ", y.shape)
        x_in=torch.cat((x, y), 1)  
        e0 = self.eninput(x_in)

        # print("e0", e0.shape)

        e0 = self.ec1(e0)
        es1 = self.ec2(e0)   #strided
        # print("e0", e0.shape)
        # print("es1", es1.shape)

        e1 = self.ec3(es1)   
        es2 = self.ec4(e1)   #strided
        # print("e1", e1.shape)
        # print("es2", es2.shape)

        e2 = self.ec5(es2)
        es3 = self.ec6(e2)   #strided
        # print("e2", e2.shape)
        # print("es3", es3.shape)

        

        d0 = self.dc1(es3)
        # print("d0", d0.shape)

        d0 = torch.add(self.up1(d0), e2)
        # print("d0", d0.shape)

        d1 = self.dc2(d0)
        d1 = torch.add(self.up2(d1), e1)
        # print("d1", d1.shape)

        d2 = self.dc3(d1)
        d2 = torch.add(self.up3(d2), e0)
        # print("d2", d2.shape)

        output = self.dc4(d2)
        return output

torch.cuda.empty_cache()
model = Admir_Deformable_UNet(2,3,16).cuda() # assining cuda to model

for X,Y in training_generator:
  X = X.cuda().float()
  Y = Y.cuda().float()
  print(X.shape)
  print(Y.shape)
  out = model(X, Y)
  print(out.shape)
  print("========== ============== =============")
  print()
  break;

del model, X, Y, out

"""# Loss Function NCC

Reference: https://github.com/yuta-hi/pytorch_similarity
"""

def normalized_cross_correlation(x, y, return_map, reduction='mean', eps=1e-8):
    """ N-dimensional normalized cross correlation (NCC)
    Args:
        x (~torch.Tensor): Input tensor.
        y (~torch.Tensor): Input tensor.
        return_map (bool): If True, also return the correlation map.
        reduction (str, optional): Specifies the reduction to apply to the output:
            ``'mean'`` | ``'sum'``. Defaults to ``'sum'``.
        eps (float, optional): Epsilon value for numerical stability. Defaults to 1e-8.
    Returns:
        ~torch.Tensor: Output scalar
        ~torch.Tensor: Output tensor
    """

    shape = x.shape
    b = shape[0]

    # reshape
    x = x.view(b, -1)
    y = y.view(b, -1)

    # mean
    x_mean = torch.mean(x, dim=1, keepdim=True)
    y_mean = torch.mean(y, dim=1, keepdim=True)

    # deviation
    x = x - x_mean
    y = y - y_mean

    dev_xy = torch.mul(x,y)
    dev_xx = torch.mul(x,x)
    dev_yy = torch.mul(y,y)

    dev_xx_sum = torch.sum(dev_xx, dim=1, keepdim=True)
    dev_yy_sum = torch.sum(dev_yy, dim=1, keepdim=True)

    ncc = torch.div(dev_xy + eps / dev_xy.shape[1],
                    torch.sqrt( torch.mul(dev_xx_sum, dev_yy_sum)) + eps)
    ncc_map = ncc.view(b, *shape[1:])

    # reduce
    if reduction == 'mean':
        ncc = torch.mean(torch.sum(ncc, dim=1))
    elif reduction == 'sum':
        ncc = torch.sum(ncc)
    else:
        raise KeyError('unsupported reduction type: %s' % reduction)

    if not return_map:
        return ncc
    
    if (torch.isclose(torch.tensor([-1.0]).to("cuda"), ncc).any()):
      ncc = ncc + torch.tensor([0.01]).to("cuda")

    elif (torch.isclose(torch.tensor([1.0]).to("cuda"), ncc).any()):
      ncc = ncc - torch.tensor([0.01]).to("cuda")

    return ncc, ncc_map

class NormalizedCrossCorrelation(nn.Module):
    """ N-dimensional normalized cross correlation (NCC)
    Args:
        eps (float, optional): Epsilon value for numerical stability. Defaults to 1e-8.
        return_map (bool, optional): If True, also return the correlation map. Defaults to False.
        reduction (str, optional): Specifies the reduction to apply to the output:
            ``'mean'`` | ``'sum'``. Defaults to ``'mean'``.
    """
    def __init__(self,
                 eps=1e-8,
                 return_map=False,
                 reduction='mean'):

        super(NormalizedCrossCorrelation, self).__init__()

        self._eps = eps
        self._return_map = return_map
        self._reduction = reduction

    def forward(self, x, y):

        return normalized_cross_correlation(x, y,self._return_map, self._reduction, self._eps)

"""# Regularizer - DVF edge smoothness"""

class Grad:
    """
    N-D gradient loss.
    """

    def __init__(self, penalty='l1', loss_mult=None):
        self.penalty = penalty
        self.loss_mult = loss_mult
        super(Grad, self).__init__()

    def loss(self, _, y_pred):
        dy = torch.abs(y_pred[:, :, 1:, :, :] - y_pred[:, :, :-1, :, :])
        dx = torch.abs(y_pred[:, :, :, 1:, :] - y_pred[:, :, :, :-1, :])
        dz = torch.abs(y_pred[:, :, :, :, 1:] - y_pred[:, :, :, :, :-1])

        if self.penalty == 'l2':
            dy = dy * dy
            dx = dx * dx
            dz = dz * dz

        d = torch.mean(dx) + torch.mean(dy) + torch.mean(dz)
        grad = d / 3.0

        if self.loss_mult is not None:
            grad *= self.loss_mult
        return grad

"""# Training Deformable Model"""

deformable_model = Admir_Deformable_UNet(2,3,16)
deformable_model.cuda()

stn_deformable = SpatialTransformer(size=(128, 128, 128), is_affine=False)
stn_deformable.cuda()


for param in stn_deformable.parameters():
  param.requires_grad = False
  param.volatile=True

# Do not run this while running first time as check point would not be created
# Run this if you need to resume training (not start from scratch) from a particular checkpoint
checkpoint = torch.load('/content/drive/My Drive/Image_Registration_Project/fullmodel2/complete_admir_8.pth')

deformable_model.load_state_dict(checkpoint['deformable_model'])

lr=1e-4
similarity_loss = NormalizedCrossCorrelation()
smoothness_loss = Grad(penalty='l2')

# PATHS OF MODEL MUST BE CHANGED ACCORDINGLY TO YOUR DRIVE

optimizer = torch.optim.Adam(list(deformable_model.parameters() ), lr=lr ) 
model_dir = '/content/drive/My Drive/Image_Registration_Project/admir_intramodal_model/test_model'

if not os.path.isdir(model_dir):
  os.mkdir(model_dir)

# X --> fixed and Y --> moving
def fullmodel_one_epoch_run(epoch=1):
  cc_loss_lst = []
  smoothness_loss_lst = []
  example_number = 0
  for X,Y in training_generator:

    X = X.cuda().float()
    Y = Y.cuda().float()
    dvf = deformable_model(X, Y) 
    
    fully_warped_image =  stn_deformable(Y,dvf)  

    deformable_loss = similarity_loss(X, fully_warped_image) 
    smooth_loss = smoothness_loss.loss("",dvf)

    total_loss = - 1.0 * deformable_loss + 1.0 * smooth_loss
    optimizer.zero_grad()          
    total_loss.backward() 
    optimizer.step() 

    del X, Y, dvf, fully_warped_image

    example_number = example_number + 1
    cc_loss_lst.append(deformable_loss.detach().cpu().numpy().item())
    smoothness_loss_lst.append(smooth_loss.detach().cpu().numpy().item())


  if (epoch%25 == 0):
    modelname = model_dir + '/' + "complete_admir_" + str(epoch) + '.pth'
    torch.save({"deformable_model": deformable_model.state_dict()}, modelname)
    print("epoch: {}".format(epoch))
    print("Losses of only last batch: {} and {}".format(deformable_loss, smooth_loss))
    print("Epoch Average Losses: {}, {}".format(sum(cc_loss_lst)/len(cc_loss_lst), sum(abs(x) for x in smoothness_loss_lst)/len(smoothness_loss_lst) ) )
    print("Saving model checkpoints")
    print("======= =============== ===========")
    print()
  else:
    print("epoch: {}".format(epoch))
    print("Losses of only last batch: {} and {}".format(deformable_loss, smooth_loss))
    print("Epoch Average Losses: {}, {}".format(sum(cc_loss_lst)/len(cc_loss_lst), sum(abs(x) for x in smoothness_loss_lst)/len(smoothness_loss_lst) ) )
    print("======= =============== ===========")
    print()

epochs=2
start_time = time.time()
for e in range(epochs):
  fullmodel_one_epoch_run(epoch=e)
 
  end_time = time.time()
  print("Total time taken: {} minutes".format((end_time-start_time)/60.0))



"""# Validating Deformable Model"""

deformable_model_inference = Admir_Deformable_UNet(2,3,16)
deformable_model_inference.cuda()

stn_deformable_inference = SpatialTransformer(size=(128, 128, 128), is_affine=False)
stn_deformable_inference.cuda()

for param in stn_deformable_inference.parameters():
  param.requires_grad = False
  param.volatile=True

model_dir = '/content/drive/My Drive/Image_Registration_Project/admir_intramodal_model/test_model'
output_image_folder = "/content/drive/My Drive/Image_Registration_Project/admir_intramodal_model/output_images"

# Suraj - Please change the path of checkpoint to latest checkpoint
checkpoint = torch.load(os.path.join(model_dir, 'complete_admir_0.pth'), map_location=torch.device('cuda'))

deformable_model_inference.load_state_dict(checkpoint['deformable_model'])

stn_deformable_inference.eval()
deformable_model_inference.eval()

def fullmodel_inference_loop(epoch=1):
  example_number = 0
  counter = 0
  for X,Y in validation_generator:

    X = X.float().to("cuda")
    Y = Y.float().to("cuda")

    dvf = deformable_model_inference(X, Y) 
    
    fully_warped_image =  stn_deformable_inference(Y,dvf)  
    
    full_warped_np = fully_warped_image.detach().to("cpu").numpy()
    full_warped_nb = nb.Nifti1Image(full_warped_np[0,0,:,:,:], np.eye(4))
    nb.save(full_warped_nb, os.path.join(output_image_folder,os.path.basename(file_names_t1[counter])[:-7]+"_F_"+os.path.basename(file_names_t2[counter])[:-7]+"_M.nii.gz"))

    counter = counter + 1
    print(counter)
    del X, Y, fully_warped_image, dvf
    torch.cuda.empty_cache() 
   
    example_number = example_number + 1
   

# Remove below condition if you want to run validation for all the images
    if(counter > 3):
      break;

fullmodel_inference_loop()

fully_warped_img = nb.load("/content/drive/My Drive/Image_Registration_Project/admir_intramodal_model/output_images/IXI165-HH-1589-T1_F_IXI247-Guys-0838-T1_M.nii.gz")
warped_img_np = fully_warped_img.dataobj

mynb_np1 = imgnorm(load_4D(file_names_t1[0]))
mynb_np1 = mynb_np1[0,:,:,:]
mynb_np2 = imgnorm(load_4D(file_names_t2[0]))
mynb_np2 = imgnorm(mynb_np2[0,:,:,:])
vol_shape = (128, 128, 128)
print(file_names_t1[0])
print(file_names_t2[0])

mid_slices_fixed = [np.take(mynb_np1, 50, axis=d) for d in range(3)]
mid_slices_fixed[1] = np.rot90(mid_slices_fixed[1], 1)
mid_slices_fixed[2] = np.rot90(mid_slices_fixed[2], -1)

mid_slices_moving = [np.take(mynb_np2, 50, axis=d) for d in range(3)]
mid_slices_moving[1] = np.rot90(mid_slices_moving[1], 1)
mid_slices_moving[2] = np.rot90(mid_slices_moving[2], -1)

mid_slices_pred = [np.take(warped_img_np, 50, axis=d) for d in range(3)]
mid_slices_pred[1] = np.rot90(mid_slices_pred[1], 1)
mid_slices_pred[2] = np.rot90(mid_slices_pred[2], -1)
ne.plot.slices(mid_slices_fixed + mid_slices_moving + mid_slices_pred, cmaps=['gray'], do_colorbars=True, grid=[3,3], titles=["Saggital_Fixed", "Axial_Fixed", "Coronal_Fixed", "Saggital_Moving", "Axial_Moving", "Coronal_Moving", "Saggital_Warped", "Axial_Warped", "Coronal_Warped"])



"""# Boundary - Below sections are not of use any more

# Building Affine Model - Do not Use or Run - Waste
"""

class Admir_Affine_Encoder(nn.Module):
    def __init__(self, in_channel, start_channel, num_conv_blocks=6):
        self.in_channel = in_channel
        self.start_channel = start_channel
        self.num_conv_blocks = num_conv_blocks
        self.kernel_size = 3
        self.stride = 2
        self.padding = 1
        self.bias=True
        self.encoder_layer_list = []
        super(Admir_Affine_Encoder, self).__init__()
        self.encoder0 = nn.Sequential(nn.Conv3d(self.in_channel, self.start_channel, self.kernel_size, 
                                                stride=self.stride, padding=self.padding, bias=self.bias ),
                            nn.BatchNorm3d(self.start_channel),
                            nn.LeakyReLU())
        self.encoder1 = nn.Sequential(nn.Conv3d(self.start_channel, self.start_channel*2, self.kernel_size, 
                                                stride=self.stride, padding=self.padding, bias=self.bias ),
                            nn.BatchNorm3d(self.start_channel*2),
                            nn.LeakyReLU())
        
        self.encoder2 = nn.Sequential(nn.Conv3d(self.start_channel*2, self.start_channel*3, self.kernel_size, 
                                                stride=self.stride, padding=self.padding, bias=self.bias ),
                            nn.BatchNorm3d(self.start_channel*3),
                            nn.LeakyReLU())
        
        self.encoder3 = nn.Sequential(nn.Conv3d(self.start_channel*3, self.start_channel*4, self.kernel_size, 
                                                stride=self.stride, padding=self.padding, bias=self.bias ),
                            nn.BatchNorm3d(self.start_channel*4),
                            nn.LeakyReLU())
        
        self.encoder4 = nn.Sequential(nn.Conv3d(self.start_channel*4, self.start_channel*5, self.kernel_size, 
                                                stride=self.stride, padding=self.padding, bias=self.bias ),
                            nn.BatchNorm3d(self.start_channel*5),
                            nn.LeakyReLU())


        # self.create_model()

    def affine_conv_block(self, in_channels, out_channels, kernel_size = 3, stride = 2, padding = 1, bias=True ):
      layer = nn.Sequential(nn.Conv3d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, bias=bias ),
                            nn.BatchNorm3d(out_channels),
                            nn.LeakyReLU())
      layer.to("cuda")
      return layer
    

    def create_model(self):
      for i in range(self.num_conv_blocks):
          if(i == 0):
            lyr = self.affine_conv_block(in_channels = self.in_channel, out_channels = self.start_channel)
            lyr.to("cuda")
            self.encoder_layer_list.append(lyr)
          else:
            lyr = self.affine_conv_block(in_channels= self.start_channel * i, out_channels = self.start_channel * (i+1))
            lyr.to("cuda")
            self.encoder_layer_list.append(lyr)

    def forward(self, x, y):
      # print("x,y", x.shape, "  ", y.shape)
      x_in=torch.cat((x, y), 1)
      #e0 = self.encoder_layer_list[0](x_in)
      e0 = self.encoder0(x_in)
      e1 = self.encoder1(e0)
      e2 = self.encoder2(e1)
      e3 = self.encoder3(e2)
      e4 = self.encoder4(e3)

      #e1 = self.encoder_layer_list[1](e0)
      """
      if(torch.isnan(e1).any()):
        print("e1 is nan")
        print(e0)
        print()

      #e2 = self.encoder_layer_list[2](e1)

      if(torch.isnan(e2).any()):
        print("e2 is nan")
        print(e1)

      #e3 = self.encoder_layer_list[3](e2)

      if(torch.isnan(e3).any()):
        print("e3 is nan")
        print(e2)

      #e4 = self.encoder_layer_list[4](e3)

      if(torch.isnan(e4).any()):
        print("e4 is nan")
        print(e3)
      """
      
      return e4

affine_conv_model = Admir_Affine_Encoder(in_channel=2, start_channel=8, num_conv_blocks=5).to("cuda")

######## To print layer outputs ########
class PrintLayer(nn.Module):
    def __init__(self):
        super(PrintLayer, self).__init__()
                    
    def forward(self, x):
        # Do your print / debug stuff here
        if(torch.isnan(x).any()):
          print(x.shape)
          print(x)
        return x

class Admir_Affine_Translation_Output(nn.Module):
  def __init__(self, in_units, out_units=128, dropout_prob = 0.2):
    
    self.in_units = in_units
    self.out_units = out_units
    self.dropout_prob = dropout_prob
    super(Admir_Affine_Translation_Output, self).__init__()
    self.trns_ob = nn.Sequential(
          nn.Linear(in_features = in_units, out_features= 128),
          nn.Dropout(p=self.dropout_prob),
          nn.Linear(in_features = 128, out_features= 3 )
          )

  def forward(self, input_tnsr):
    ip = input_tnsr.flatten(start_dim=1, end_dim=4)
    translation_output = self.trns_ob(ip)
    return translation_output

  def translation_output_block(self, in_units, out_units):
    layer = nn.Sequential(
          nn.Linear(in_features = in_units, out_features= out_units),
          PrintLayer(),
          nn.Dropout(p=self.dropout_prob),
          
          nn.Linear(in_features=out_units, out_features= out_units//2),
          #PrintLayer(),
          nn.Dropout(p=self.dropout_prob),

          nn.Linear(in_features=out_units//2, out_features= out_units//4),
          #PrintLayer(),
          nn.Dropout(p=self.dropout_prob),

          nn.Linear(in_features=out_units//4, out_features= out_units//8),
          #PrintLayer(),
          nn.Dropout(p=self.dropout_prob),

          nn.Linear(in_features=out_units//8, out_features= 3)
          #PrintLayer()
          ).to("cuda")

    return layer

class Admir_Affine_Shear_Output(nn.Module):
  def __init__(self, in_units, out_units=128, dropout_prob = 0.2):
    
    self.in_units = in_units
    self.out_units = out_units
    self.dropout_prob = dropout_prob
    super(Admir_Affine_Shear_Output, self).__init__()
    self.rss_ob = nn.Sequential(
          nn.Linear(in_features = in_units, out_features= 128),
          nn.Dropout(p=self.dropout_prob),
          nn.Linear(in_features = 128, out_features= 9),
          nn.Tanh())
  
  def forward(self, input_tnsr):
    ip = input_tnsr.flatten(start_dim=1, end_dim=4)
    rotate_scale_shear_output = self.rss_ob(ip)
    return rotate_scale_shear_output

affine_translation_output = Admir_Affine_Translation_Output( in_units= 2560).to("cuda")
affine_shear_output = Admir_Affine_Shear_Output( in_units= 2560).to("cuda")

counter = 0
for X,Y in training_generator:
  X = X.cuda().float()
  Y = Y.cuda().float()
  print(X.shape)
  print(Y.shape)
  conv_out = affine_conv_model(X, Y)
  print(conv_out.shape)
  translation_params = affine_translation_output(conv_out)
  rotation_shear_scale_params = affine_shear_output(conv_out)
  print(translation_params.shape)
  print(translation_params)
  print()
  print(rotation_shear_scale_params.shape)
  print(rotation_shear_scale_params)
  print("========== ============== =============")
  print()
  counter = counter + 1
  if(counter > 3):
    break;

"""# Loss Function Mutual Information - Do not use this"""

num_bins = 10
bin_centers = np.linspace(0, 0.7, num_bins*2+1)[1::2]
sigma = np.mean(np.diff(bin_centers)) * 0.5
preterm = torch.tensor(1 / (2 * np.square(sigma)))
dim_prod = 128*128*128
print(num_bins)
print(bin_centers)
print(sigma)
print(preterm)

def mutual_information(y_true, y_pred, sigma, preterm, bin_centers, nb_voxels, dim_prod=128 * 128 * 128):

  y_true = torch.reshape(y_true, (y_true.shape[0], dim_prod, 1) )
  y_pred = torch.reshape(y_pred, (y_pred.shape[0], dim_prod, 1) )

  vbc = torch.tensor(bin_centers)
  vbc = vbc.unsqueeze(0)
  vbc = vbc.unsqueeze(0)

  # compute image terms
  I_a = torch.exp(- preterm * torch.square(y_true  - vbc))
  I_a = I_a/torch.sum(I_a)

  #print(I_a)

  I_b = torch.exp(- preterm * torch.square(y_pred  - vbc))
  I_b = I_b/torch.sum(I_b)

  #print(I_b)

  # compute probabilities
  I_a_permute = I_a.permute(0,2,1)
  pab = torch.matmul(I_a_permute, I_b) 
  #print(pab)
  #print(torch.sum(pab))

  # should be the right size now, nb_labels x nb_bins
  pab = pab/nb_voxels
  pa = torch.mean(I_a, 1, keepdims=True)
  pb = torch.mean(I_b, 1, keepdims=True)

  papb = torch.matmul(pa.permute(0,2,1), pb) + torch.finfo(torch.float32).eps
  #print(papb)

  mi = torch.sum(torch.sum(pab * torch.log(pab/papb + torch.finfo(torch.float32).eps), 1), 1)

  return -mi

class NormalizedMutualInformation(nn.Module):

    def __init__(self,
                 dim_prod = 128*128*128,
                 num_bins = 10,
                 bin_centers = np.linspace(0, 0.7, 10*2+1)[1::2],
                 sigma = np.mean(np.diff(bin_centers)) * 0.5,
                 preterm = torch.tensor(1 / (2 * np.square(sigma))),
                 nb_voxels = torch.tensor([dim_prod], dtype=torch.float32)
                 ):

        super(NormalizedMutualInformation, self).__init__()

        self.dim_prod = dim_prod
        self.num_bins = num_bins
        self.bin_centers = bin_centers
        self.sigma = sigma
        self.preterm = preterm
        self.nb_voxels = nb_voxels

    def forward(self, x, y):

        return mutual_information(x,y, self.sigma, self.preterm, self.bin_centers, self.nb_voxels, self.dim_prod)

nmi_loss = NormalizedMutualInformation()
counter = 0
for X,Y in training_generator:

  print(nmi_loss(X, X))
  counter = counter + 1
  if(counter > 2):
    break;

"""# Miscellaneous Losses

**Determinant loss**: determinant of coarsely warped image should be close to one to make it volume preserving, if it scales too much or less then its a problem

**Orthogonality preserving loss**: Tries to preserves angle and length between voxels of interest when transformed to another reference
"""

def det3x3(M):
    M = [[M[:, i, j] for j in range(3)] for i in range(3)]
    return sum([
                M[0][0] * M[1][1] * M[2][2],
                M[0][1] * M[1][2] * M[2][0],
                M[0][2] * M[1][0] * M[2][1]
            ]) - sum([
                M[0][0] * M[1][2] * M[2][1],
                M[0][1] * M[1][0] * M[2][2],
                M[0][2] * M[1][1] * M[2][0]
            ])

def elem_sym_polys_of_eigen_values(M):
            M = [[M[:, i, j] for j in range(3)] for i in range(3)]
            sigma1 = sum([M[0][0], M[1][1], M[2][2]])
            sigma2 = sum([
                M[0][0] * M[1][1],
                M[1][1] * M[2][2],
                M[2][2] * M[0][0]
            ]) - sum([
                M[0][1] * M[1][0],
                M[1][2] * M[2][1],
                M[2][0] * M[0][2]
            ])
            sigma3 = sum([
                M[0][0] * M[1][1] * M[2][2],
                M[0][1] * M[1][2] * M[2][0],
                M[0][2] * M[1][0] * M[2][1]
            ]) - sum([
                M[0][0] * M[1][2] * M[2][1],
                M[0][1] * M[1][0] * M[2][2],
                M[0][2] * M[1][1] * M[2][0]
            ])
            return sigma1, sigma2, sigma3

class VolumePreservingLoss(nn.Module):
    def __init__(self, onetensor = torch.tensor([1.0, 1.0]).to("cuda")):
        super(VolumePreservingLoss, self).__init__()
        self.onetnsr = onetensor
        self.mse_loss = torch.nn.MSELoss(reduce="mean")

    def forward(self, M):
      #print(det3x3(M))
      #print(self.onetnsr)      
      det_loss = self.mse_loss(det3x3(M), self.onetnsr)
      return det_loss

class OrthogonalTransformLoss(nn.Module):
    def __init__(self):

      super(OrthogonalTransformLoss, self).__init__()
      self.eps = 1e-7
      self.I = [[[1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0]]]
      self.epsI = torch.FloatTensor([[[self.eps * elem for elem in row] for row in Mat] for Mat in self.I]).to("cuda")

    def forward(self, M):
      C = torch.matmul(M, torch.transpose(M,1,2)) + self.epsI
      s1, s2, s3 = elem_sym_polys_of_eigen_values(C)
      ortho_loss = torch.sum(s1 + (1 + self.eps) * (1 + self.eps) * s2 / s3 - 3 * 2 * (1 + self.eps))
      return ortho_loss

M = torch.randn(size=(2,3,3)).to("cuda")
vpl = VolumePreservingLoss()
otl = OrthogonalTransformLoss()
M

vpl(M)

otl(M)

"""# Aggregation Affine DVF and Deformable DVF

![image.png](data:image/png;base64,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)

**Steps:**
 According to eq. 3, warp DVF of affine(g1) with DVF of deformable(g2), and then add the result to DVF of deformable(g2).
"""

affine_conv_model = Admir_Affine_Encoder(in_channel=2, start_channel=8, num_conv_blocks=5)
affine_output_model = Admir_Affine_Output( in_units= 2560)
stn_affine = SpatialTransformer(size=(128, 28, 128), is_affine=True, theta=torch.randn(size=(2, 3, 4)))

deformable_model = Admir_Deformable_UNet(2,3,16)

stn_deformable = SpatialTransformer(size=(128, 128, 128), is_affine=False)
stn_aggregation = SpatialTransformer(size=(128, 128, 128), is_affine=False)

stn_fully_warped = SpatialTransformer(size=(128, 128, 128), is_affine=False)

stn_affine.grid.shape

final_warped_image = stn_deformable(torch.randn(size=(2,3,128,128,128)), torch.randn(size=(2,3,128,128,128)) )

final_warped_image.shape

batch_size = 2


# X --> fixed and Y --> moving

for X,Y in training_generator:

  # X = X.cuda().float()
  # Y = Y.cuda().float()

  print("X shape: {}".format(X.shape))
  print("Y shape: {}".format(Y.shape))
  affine_conv_out = affine_conv_model(X, Y)
  # print(affine_conv_out.shape)

  affine_output_out = affine_output_model(affine_conv_out)
  print(affine_output_out[0].shape)
  print(affine_output_out[1].shape)

  print("========== ============== =============")
  print()

  affine_tnsr = torch.cat((affine_output_out[1], affine_output_out[0]), 1)
  theta = torch.reshape(affine_tnsr, (batch_size, 3, 4))

  
  # Suraj: Set the new theta and reshape it
  stn_affine.theta = theta
  print("========== ============== =============")
  print("theta shape: {}".format(theta.shape))
  print(theta)

  # print("========== ============== =============")
  # grid = F.affine_grid(theta, (2, 1, 128, 128, 128))
  # print(grid.shape)
  # print(grid)

  # Suraj: Added coarsely warped image, changed dvf_final calculation
  # DVF1 shape was [batch size, 128, 128, 128, 3] --> permuted order of columns to match deformable dvf 2 [batch size, 3, 128, 128, 128]
  coarsely_warped_image = stn_affine(Y)
  dvf_1 = stn_affine.grid
  dvf_1 = dvf_1.permute(0, 4, 1, 2, 3)

  print("======= =============== ===========")
  print("Coarsely warped image shape: {}".format(coarsely_warped_image.shape))
  print("dvf 1 shape: {}".format(dvf_1.shape))
  print("=========== ============= ==========")
  print()

  print(" ==== Starting deformable warping =======")
  dvf_2 = deformable_model(coarsely_warped_image, Y) 
  print("deformable field dvf 2 shape: {}".format(dvf_2.shape))
  print("========== ============== =============")
  print()

  print(" interim warp ")
  interim_warp_field = stn_aggregation(dvf_1,dvf_2)
  print("interim warp field shape: {}".format(interim_warp_field.shape))
  print("========== ============== =============")
  print()

  
  dvf_final = dvf_2 + interim_warp_field
  print("final dvf shape: {}".format(dvf_final.shape))
  print("========== ============== =============")
  print()

  fully_warped_image =  stn_fully_warped(Y,dvf_final)  #final fully warped image , i.e. equation 4 of the image
  print("final warped image shape: {}".format(fully_warped_image.shape))
  print("========== ============== =============")
  print()
  

  del X
  del Y
  del interim_warp_field
  del dvf_1
  del dvf_2
  del dvf_final
  del fully_warped_image

  """
  # something is missing here may be: TBD

  dvf_1 = F.grid_sample(Y, grid)    #named as g1 in the image
  print(dvf_1.shape)
  print("========== ============== =============")
  print()
  
  dvf_2 = deformable_model(dvf_1, Y)     #named as g2 in the image
  print(dvf_2.shape)
  print("========== ============== =============")
  print()
  
  dvf_final = dvf_2 + stn_aggregation(dvf_1,dvf_2)    # g1 * g2,  aggregation of DVF (Affine) and DVF (Deformable)
  print(dvf_final.shape)
  print("========== ============== =============")
  print()

  fully_warped_image =  stn_fully_warped(Y,dvf_final)  #final fully warped image , i.e. equation 4 of the image
  print(fully_warped_image.shape)
  print("========== ============== =============")
  print()
  """

"""# Training Affine Network to sanity check"""

affine_conv_model = Admir_Affine_Encoder(in_channel=2, start_channel=8, num_conv_blocks=5)
affine_conv_model.cuda()

affine_output_model = Admir_Affine_Output( in_units= 2560)
affine_output_model.cuda()


affine_matrix = torch.from_numpy(np.array([ [ [1.0, 0.0, 0.0, 0.0], [ 0.0, 1.0, 0.0, 0.0], [0.0,  0.0, 1.0, 0.0] ] , 
                                            [ [1.0, 0.0, 0.0, 0.0], [ 0.0, 1.0, 0.0, 0.0], [0.0,  0.0, 1.0, 0.0] ]  ], dtype="float32" )).cuda()
stn_affine = SpatialTransformer(size=(128, 28, 128), is_affine=True, theta=affine_matrix)
stn_affine.cuda()

# Addd deformable part
deformable_model = Admir_Deformable_UNet(2,3,16)
deformable_model.cuda()

stn_deformable = SpatialTransformer(size=(128, 128, 128), is_affine=False)
stn_deformable.cuda()

stn_aggregation = SpatialTransformer(size=(128, 128, 128), is_affine=False)
stn_aggregation.cuda()

stn_fully_warped = SpatialTransformer(size=(128, 128, 128), is_affine=False)
stn_fully_warped.cuda()


lr=2e-4
iteration=2
alpha = beta = gamma = 1.0

for param in stn_affine.parameters():
  param.requires_grad = False
  param.volatile=True

for param in stn_deformable.parameters():
  param.requires_grad = False
  param.volatile=True

for param in stn_aggregation.parameters():
  param.requires_grad = False
  param.volatile=True

for param in stn_fully_warped.parameters():
  param.requires_grad = False
  param.volatile=True

affine_matrix

similarity_loss = NormalizedCrossCorrelation()
smooth_loss = Smoothnessloss()

optimizer = torch.optim.Adam( list( affine_conv_model.parameters()) + list( affine_output_model.parameters() ) , lr=lr ) 
model_dir = '/content/drive/My Drive/Image_Registration_Project/Model'

if not os.path.isdir(model_dir):
  os.mkdir(model_dir)

n_checkpoint=4000
lossall = np.zeros((5,iteration))
step=0

batch_size = 2

# X --> fixed and Y --> moving
def affine_one_epoch_run(epoch=1):
  for X,Y in training_generator:

    X = X.cuda().float()
    Y = Y.cuda().float()

    #print("X shape: {}".format(X.shape))
    #print("Y shape: {}".format(Y.shape))
    affine_conv_out = affine_conv_model(X, Y)
    # print(affine_conv_out.shape)

    affine_output_out = affine_output_model(affine_conv_out)
    #print(affine_output_out[0].shape)
    #print(affine_output_out[1].shape)

    #print("========== ============== =============")
    #print()

    rss_rt = torch.reshape(affine_output_out[1],(batch_size, 3, 3))
    t_rt = torch.reshape(affine_output_out[0], (batch_size, 3, 1))

    theta = torch.cat((rss_rt, t_rt), 2)
    #print(theta)
    #print("========== ============== =============")
    #print()
    # theta = affine_matrix - theta
    #print(theta)
    
    # Suraj: Set the new theta and reshape it
    stn_affine.theta = theta
    #print("========== ============== =============")
    #print("theta shape: {}".format(theta.shape))
    #print(theta)

    # print("========== ============== =============")
    # grid = F.affine_grid(theta, (2, 1, 128, 128, 128))
    # print(grid.shape)
    # print(grid)

    # Suraj: Added coarsely warped image, changed dvf_final calculation
    # DVF1 shape was [batch size, 128, 128, 128, 3] --> permuted order of columns to match deformable dvf 2 [batch size, 3, 128, 128, 128]
    coarsely_warped_image = stn_affine(Y)
    dvf_1 = stn_affine.grid
    dvf_1 = dvf_1.permute(0, 4, 1, 2, 3)

    #print("======= =============== ===========")
    #print("Coarsely warped image shape: {}".format(coarsely_warped_image.shape))
    #print("dvf 1 shape: {}".format(dvf_1.shape))
    #print("=========== ============= ==========")
    #print()

    affine_loss = similarity_loss(X, coarsely_warped_image) 
    optimizer.zero_grad()          
    affine_loss.backward()           
    optimizer.step() 

    torch.cuda.empty_cache()

    del X
    del Y
    del coarsely_warped_image
    del dvf_1
  



  modelname = model_dir + '/' + "affine_admir_" + str(epoch) + '.pth'
  torch.save({"affine_conv_model": affine_conv_model.state_dict(), "affine_output_model": affine_output_model.state_dict()}, modelname)
  print("epoch: {}".format(epoch))
  print("Loss: {}".format(affine_loss))
  print("theta: {}".format(stn_affine.theta))
  print("Saving model checkpoints")
  print("======= =============== ===========")
  print()

epochs = 10
for e in range(epochs):
  affine_one_epoch_run(epoch=e)

"""# Affine Inference"""

affine_conv_model_inference = Admir_Affine_Encoder(in_channel=2, start_channel=8, num_conv_blocks=5)
affine_conv_model_inference.cuda()

affine_output_model_inference = Admir_Affine_Output( in_units= 2560)
affine_output_model_inference.cuda()

stn_affine_inference = SpatialTransformer(size=(128, 28, 128), is_affine=True, theta=torch.randn(size=(1, 3, 4)), affine_image_size =  (1, 1, 128, 128, 128))
stn_affine_inference.cuda()

checkpoint = torch.load('/content/drive/My Drive/Image_Registration_Project/fullmodel2/complete_admir_9.pth')
affine_conv_model_inference.load_state_dict(checkpoint['affine_conv_model'])
affine_output_model_inference.load_state_dict(checkpoint['affine_output_model'])
affine_conv_model_inference.eval()
affine_output_model_inference.eval()
affine_matrix_inference = torch.from_numpy(np.array([ [ [1.0, 0.0, 0.0, 0.0], [ 0.0, 1.0, 0.0, 0.0], [0.0,  0.0, 1.0, 0.0] ]  ], dtype="float32" )).cuda()

counter=0

batch_size=1
def inference_loop(epoch=1, batch_size=1, counter=0):
  for X,Y in validation_generator:

    X = X.cuda().float()
    Y = Y.cuda().float()

    #print("X shape: {}".format(X.shape))
    #print("Y shape: {}".format(Y.shape))
    affine_conv_out = affine_conv_model_inference(X, Y)
    # print(affine_conv_out.shape)

    affine_output_out = affine_output_model_inference(affine_conv_out)
    #print(affine_output_out[0].shape)
    #print(affine_output_out[1].shape)

    #print("========== ============== =============")
    #print()

    rss_rt = torch.reshape(affine_output_out[1],(batch_size, 3, 3))
    t_rt = torch.reshape(affine_output_out[0], (batch_size, 3, 1))

    theta = torch.cat((rss_rt, t_rt), 2)


    theta = affine_matrix_inference - theta
    
    # Suraj: Set the new theta and reshape it
    stn_affine_inference.theta = theta
    #print("========== ============== =============")
    #print("theta shape: {}".format(theta.shape))
    print(theta)

    # print("========== ============== =============")
    # grid = F.affine_grid(theta, (2, 1, 128, 128, 128))
    # print(grid.shape)
    # print(grid)

    # Suraj: Added coarsely warped image, changed dvf_final calculation
    # DVF1 shape was [batch size, 128, 128, 128, 3] --> permuted order of columns to match deformable dvf 2 [batch size, 3, 128, 128, 128]
    coarsely_warped_image = stn_affine_inference(Y)
    dvf_1 = stn_affine_inference.grid
    dvf_1 = dvf_1.permute(0, 4, 1, 2, 3)

    output_warped_tensor = coarsely_warped_image.detach().to("cpu")
    #dvf_1_warped_tensor = dvf_1.detach().to("cpu")
    output_warped_np = output_warped_tensor.numpy()
    #dvf_1_warped_np = dvf_1_warped_tensor.numpy()

    output_warped_nb = nb.Nifti1Image(output_warped_np[0,0,:,:,:], np.eye(4)) 
    nb.save(output_warped_nb, '/content/drive/My Drive/Image_Registration_Project/dataset_ants_resampled/warped_images/' +  'output_warped_nb_' + str(counter) + '.nii.gz')
    #dvf_1_warped_nb = nb.Nifti1Image(dvf_1_warped_np[0,:,:,:,:], np.eye(4)) 
    #nb.save(dvf_1_warped_nb, '/content/drive/My Drive/Image_Registration_Project/dataset_ants_resampled/' +  'dvf_1_warped_nb_' + str(counter) + '.nii.gz')

    counter = counter + 1

    if(counter > 2):
      break;

    del output_warped_tensor
    del dvf_1
    #del dvf_1_warped_tensor
    del coarsely_warped_image

inference_loop()

warped_img = nb.load("/content/drive/My Drive/Image_Registration_Project/dataset_ants_resampled/warped_images/output_warped_nb_1.nii.gz")
warped_img_np = warped_img.dataobj
mynb_np1 = load_4D(file_names_t1[1])
mynb_np2 = load_4D(file_names_t1[5])

plt.imshow(warped_img_np[:, :, 25])

plt.imshow(mynb_np1[0, :, :, 32])

plt.imshow(mynb_np2[0, :, :, 25])



"""# Training Full model"""

boltn = torch.isnan(torch.from_numpy(np.array([[1.0, 0.2, np.nan], [1.0, 0.2, -0.4]]))).any()
if(boltn):
  print("true")
else:
  print("false")

affine_conv_model = Admir_Affine_Encoder(in_channel=2, start_channel=8, num_conv_blocks=5)
affine_conv_model.to("cuda")

#affine_output_model = Admir_Affine_Output( in_units= 2560)
#affine_output_model.cuda()

affine_translation_model = Admir_Affine_Translation_Output( in_units= 2560).to("cuda")
affine_shear_model = Admir_Affine_Shear_Output( in_units= 2560).to("cuda")

affine_matrix = torch.from_numpy(np.array([ [ [1.0, 0.0, 0.0, 0.0], [ 0.0, 1.0, 0.0, 0.0], [0.0,  0.0, 1.0, 0.0] ] , 
                                            [ [1.0, 0.0, 0.0, 0.0], [ 0.0, 1.0, 0.0, 0.0], [0.0,  0.0, 1.0, 0.0] ]  ], dtype="float32" )).cuda()

rss_matrix = torch.from_numpy(np.array([ [ [1.0, 0.0, 0.0], [ 0.0, 1.0, 0.0], [0.0,  0.0, 1.0] ] , 
                                         [ [1.0, 0.0, 0.0], [ 0.0, 1.0, 0.0], [0.0,  0.0, 1.0] ] ], dtype="float32" )).to("cuda")

affine_matrix_inference = torch.from_numpy(np.array([ [ [1.0, 0.0, 0.0, 0.0], [ 0.0, 1.0, 0.0, 0.0], [0.0,  0.0, 1.0, 0.0] ]  ], dtype="float32" )).cuda()


stn_affine = SpatialTransformer(size=(128, 28, 128), is_affine=True, theta=affine_matrix,  affine_image_size =  (2, 1, 128, 128, 128))
stn_affine.cuda()

# Addd deformable part
deformable_model = Admir_Deformable_UNet(2,3,16)
deformable_model.cuda()

stn_deformable = SpatialTransformer(size=(128, 128, 128), is_affine=False)
stn_deformable.cuda()

stn_aggregation = SpatialTransformer(size=(128, 128, 128), is_affine=False)
stn_aggregation.cuda()

stn_fully_warped = SpatialTransformer(size=(128, 128, 128), is_affine=False)
stn_fully_warped.cuda()

for param in stn_affine.parameters():
  param.requires_grad = False
  param.volatile=True

for param in stn_deformable.parameters():
  param.requires_grad = False
  param.volatile=True

for param in stn_aggregation.parameters():
  param.requires_grad = False
  param.volatile=True

for param in stn_fully_warped.parameters():
  param.requires_grad = False
  param.volatile=True

checkpoint = torch.load('/content/drive/My Drive/Image_Registration_Project/fullmodel2/complete_admir_8.pth')
affine_conv_model.load_state_dict(checkpoint['affine_conv_model'])
affine_translation_model.load_state_dict(checkpoint['affine_translation_model'])
affine_shear_model.load_state_dict(checkpoint['affine_shear_model'])
#affine_output_model_inference.load_state_dict(checkpoint['affine_output_model'])
deformable_model.load_state_dict(checkpoint['deformable_model'])

affine_matrix

lr=1e-4
iteration=2
alpha = 1.0
beta = 1.0
gamma = 1.0
det_mul = 1.0
ortho_mul = 1.0

similarity_loss = NormalizedCrossCorrelation()
smooth_loss = Smoothnessloss()
volume_preserving_loss = VolumePreservingLoss()
orthogonal_transformation_loss = OrthogonalTransformLoss()

optimizer = torch.optim.Adam( list( affine_conv_model.parameters()) + list( affine_translation_model.parameters() ) + 
                             list( affine_shear_model.parameters()) + list(deformable_model.parameters() ), lr=lr ) 
model_dir = '/content/drive/My Drive/Image_Registration_Project/fullmodel2'

if not os.path.isdir(model_dir):
  os.mkdir(model_dir)

n_checkpoint=4000
lossall = np.zeros((5,iteration))
step=0

batch_size = 2

# X --> fixed and Y --> moving
def fullmodel_one_epoch_run(epoch=1):
  example_number = 0
  for X,Y in training_generator:

    X = X.cuda().float()
    Y = Y.cuda().float()

    

    #print("X shape: {}".format(X.shape))
    #print("Y shape: {}".format(Y.shape))
    affine_conv_out = affine_conv_model(X, Y)
    # print(affine_conv_out.shape)

    bool_conv_out = torch.isnan(affine_conv_out).any()

    if(bool_conv_out):
      #print("Affine conv output is NaN")
      ;
      

    translation_out = affine_translation_model(affine_conv_out)
    rss_out = affine_shear_model(affine_conv_out)

    #print(affine_output_out[0].shape)
    #print(affine_output_out[1].shape)

    #print("========== ============== =============")
    #print()

    rss_rt = torch.reshape(rss_out,(batch_size, 3, 3))

    rss_rt_new = rss_rt + rss_matrix

    orthogonal_loss = orthogonal_transformation_loss(rss_rt_new)
    volume_loss = volume_preserving_loss(rss_rt_new)

    # print(orthogonal_loss)
    # print(volume_loss)
    t_rt = torch.reshape(translation_out, (batch_size, 3, 1))

    theta = torch.cat((rss_rt_new, t_rt), 2)

    bool_affine_out = torch.isnan(theta).any()

    if(bool_affine_out):
      #print("Affine matrix output is NaN")
      #print("Mean: {}".format(torch.mean(affine_conv_out)))
      #print("Min: {}".format(torch.min(affine_conv_out)))
      #print("Max: {}".format(torch.max(affine_conv_out)))      
      #print(theta)
      ;
      
      #del X
      #del Y
      #break;
    else:
      ;
      #print("====================     =============")
      #print("Mean: {}".format(torch.mean(affine_conv_out)))
      #print("Min: {}".format(torch.min(affine_conv_out)))
      #print("Max: {}".format(torch.max(affine_conv_out)))

    #theta = torch.reshape(affine_tnsr, (batch_size, 3, 4))


    # theta = affine_matrix + theta
    
    # Suraj: Set the new theta and reshape it
    stn_affine.theta = theta
    # print("========== ============== =============")
    #print("theta shape: {}".format(theta.shape))
    # print(theta)

    # print("========== ============== =============")
    # grid = F.affine_grid(theta, (2, 1, 128, 128, 128))
    # print(grid.shape)
    # print(grid)

    # Suraj: Added coarsely warped image, changed dvf_final calculation
    # DVF1 shape was [batch size, 128, 128, 128, 3] --> permuted order of columns to match deformable dvf 2 [batch size, 3, 128, 128, 128]
    coarsely_warped_image = stn_affine(Y)
    dvf_1 = stn_affine.grid
    dvf_1 = dvf_1.permute(0, 4, 1, 2, 3)

    #print("======= =============== ===========")
    #print("Coarsely warped image shape: {}".format(coarsely_warped_image.shape))
    #print("dvf 1 shape: {}".format(dvf_1.shape))
    #print("=========== ============= ==========")
    #print()

    affine_loss = similarity_loss(X, coarsely_warped_image) 
    # optimizer.zero_grad()           # clear gradients for this training step
    # affine_loss.backward()                 # backpropagation, compute gradients
    # optimizer.step() 

    

    # -----------------------Deformable part-----------------------

    # print(" ==== Starting deformable warping =======")
    dvf_2 = deformable_model(X, coarsely_warped_image) 
    # print("deformable field dvf 2 shape: {}".format(dvf_2.shape))
    # print("========== ============== =============")
    # print()

    # print(" interim warp ")
    interim_warp_field = stn_aggregation(dvf_1,dvf_2)
    # print("interim warp field shape: {}".format(interim_warp_field.shape))
    # print("========== ============== =============")
    # print()

  
    dvf_final = dvf_2 + interim_warp_field
    # print("final dvf shape: {}".format(dvf_final.shape))
    # print("========== ============== =============")
    # print()

    fully_warped_image =  stn_fully_warped(Y,dvf_final)  #final fully warped image , i.e. equation 4 of the image
    # print("final warped image shape: {}".format(fully_warped_image.shape))
    # print("========== ============== =============")
    # print()

    deformable_loss = similarity_loss(X, fully_warped_image) 
    # optimizer.zero_grad()                      
    # deformable_loss.backward()               
    # optimizer.step() 
     
    smooth_loss = smoothness_regularizer(dvf_2)

    total_loss = (- alpha * affine_loss )+ ( - beta * deformable_loss ) + ( gamma * smooth_loss ) + 0.2 * orthogonal_loss + volume_loss
    #print(total_loss)
    optimizer.zero_grad()          
    total_loss.backward() 

     
    #print("=======  loss gradients for example {} ===========".format(example_number))
    translation_wght_1 = affine_translation_model.trns_ob[0].weight
    translation_wght_2 = affine_translation_model.trns_ob[2].weight
    deformable_encoder_wght_1 = deformable_model.ec1[0].weight
    #print("Affine Loss, Deformable Loss, smooth loss Orthogonal loss and Volume Loss are {}, {}, {}, {} and {}".format(-alpha * affine_loss, -beta * deformable_loss, smooth_loss, orthogonal_loss, volume_loss))
    translation_grad_1 = affine_translation_model.trns_ob[0].weight.grad
    translation_grad_2 = affine_translation_model.trns_ob[2].weight.grad
    deformable_encoder_grad_1 = deformable_model.ec1[0].weight.grad

    #print("Min, Mean, Max and shape of gradient 1 is {}, {}, {} and {}".format(torch.min(translation_grad_1), torch.mean(translation_grad_1), torch.max(translation_grad_1), translation_grad_1.shape ))
    #print("Min, Mean, Max and shape of gradient 2 is {}, {}, {} and {}".format(torch.min(translation_grad_2), torch.mean(translation_grad_2), torch.max(translation_grad_2), translation_grad_2.shape ))
    #print("Min, Mean, Max and shape of gradient 3 is {}, {}, {} and {}".format(torch.min(deformable_encoder_grad_1), torch.mean(deformable_encoder_grad_1), torch.max(deformable_encoder_grad_1), deformable_encoder_grad_1.shape ))
    
    #print("Min, Mean, Max and shape of weight 1 is {}, {}, {} and {}".format(torch.min(translation_wght_1), torch.mean(translation_wght_1), torch.max(translation_wght_1), translation_wght_1.shape ))
    #print("Min, Mean, Max and shape of weight 2 is {}, {}, {} and {}".format(torch.min(translation_wght_2), torch.mean(translation_wght_2), torch.max(translation_wght_2), translation_wght_2.shape ))    
    #print("Min, Mean, Max and shape of weight 3 is {}, {}, {} and {}".format(torch.min(deformable_encoder_wght_1), torch.mean(deformable_encoder_wght_1), torch.max(deformable_encoder_wght_1), deformable_encoder_wght_1.shape ))
    #print(theta)
    #print("========= =========== ============ ==============")
    optimizer.step() 

    del X
    del Y
    del coarsely_warped_image
    del dvf_1
    del dvf_2
    del dvf_final
    del fully_warped_image

    example_number = example_number + 1


  if (epoch%4 == 0):
    modelname = model_dir + '/' + "complete_admir_" + str(epoch) + '.pth'
    torch.save({"affine_conv_model": affine_conv_model.state_dict(), "affine_shear_model": affine_shear_model.state_dict(),
                "affine_translation_model": affine_translation_model.state_dict(), "deformable_model": deformable_model.state_dict()}, modelname)
    print("epoch: {}".format(epoch))
    print("Losses: {}, {} and {}".format(affine_loss, deformable_loss, smooth_loss))
    print("Affine transform matrix: {}".format(stn_affine.theta))
    print("Saving model checkpoints")
    print("======= =============== ===========")
    print()

epochs = 29
for e in range(epochs):
  fullmodel_one_epoch_run(epoch=e)

"""# Infering Full Model"""

affine_conv_model_inference = Admir_Affine_Encoder(in_channel=2, start_channel=8, num_conv_blocks=5)
affine_conv_model_inference.to("cuda")

#affine_output_model_inference = Admir_Affine_Output( in_units= 2560)
#affine_output_model_inference.cuda()

affine_translation_output_inference = Admir_Affine_Translation_Output( in_units= 2560).to("cuda")
affine_shear_output_inference = Admir_Affine_Shear_Output( in_units= 2560).to("cuda")


affine_matrix_inference = torch.from_numpy(np.array([ [ [1.0, 0.0, 0.0, 0.0], [ 0.0, 1.0, 0.0, 0.0], [0.0,  0.0, 1.0, 0.0] ]  ], dtype="float32" )).cuda()
stn_affine_inference = SpatialTransformer(size=(128, 28, 128), is_affine=True, theta=torch.randn(size=(1, 3, 4)), affine_image_size =  (1, 1, 128, 128, 128))
stn_affine_inference.cuda()

# Addd deformable part
deformable_model_inference = Admir_Deformable_UNet(2,3,16)
deformable_model_inference.to("cuda")

stn_deformable_inference = SpatialTransformer(size=(128, 128, 128), is_affine=False)
stn_deformable_inference.cuda()

stn_aggregation_inference = SpatialTransformer(size=(128, 128, 128), is_affine=False)
stn_aggregation_inference.cuda()

stn_fully_warped_inference = SpatialTransformer(size=(128, 128, 128), is_affine=False)
stn_fully_warped_inference.cuda()

checkpoint = torch.load('/content/drive/My Drive/Image_Registration_Project/fullmodel2/complete_admir_28.pth')
affine_conv_model_inference.load_state_dict(checkpoint['affine_conv_model'])
affine_translation_output_inference.load_state_dict(checkpoint['affine_translation_model'])
affine_shear_output_inference.load_state_dict(checkpoint['affine_shear_model'])
#affine_output_model_inference.load_state_dict(checkpoint['affine_output_model'])
deformable_model_inference.load_state_dict(checkpoint['deformable_model'])
affine_conv_model_inference.eval()
affine_translation_output_inference.eval()
affine_shear_output_inference.eval()
#affine_output_model_inference.eval()
deformable_model_inference.eval()

counter = 0

def fullmodel_inference_loop(epoch=1, batch_size=1, counter=0):
  for X,Y in validation_generator:

    X = X.cuda().float()
    Y = Y.cuda().float()

    #print("X shape: {}".format(X.shape))
    #print("Y shape: {}".format(Y.shape))
    affine_conv_out = affine_conv_model_inference(X, Y)
    # print(affine_conv_out.shape)

    translation_out = affine_translation_output_inference(affine_conv_out)
    rss_out = affine_shear_output_inference(affine_conv_out)

    #print("========== ============== =============")
    #print()

    rss_rt = torch.reshape(rss_out,(batch_size, 3, 3))
    t_rt = torch.reshape(translation_out, (batch_size, 3, 1))

    theta = torch.cat((rss_rt, t_rt), 2)

    #affine_tnsr = torch.cat((affine_output_out[1], affine_output_out[0]), 1)
    #theta = torch.reshape(affine_tnsr, (1, 3, 4))
    theta = affine_matrix_inference + theta
    print(theta)
    # Suraj: Set the new theta and reshape it
    stn_affine_inference.theta = theta
    #print("========== ============== =============")
    #print("theta shape: {}".format(theta.shape))
    #print(theta)

    # print("========== ============== =============")
    # grid = F.affine_grid(theta, (2, 1, 128, 128, 128))
    # print(grid.shape)
    # print(grid)

    # Suraj: Added coarsely warped image, changed dvf_final calculation
    # DVF1 shape was [batch size, 128, 128, 128, 3] --> permuted order of columns to match deformable dvf 2 [batch size, 3, 128, 128, 128]
    coarsely_warped_image = stn_affine_inference(Y)
    dvf_1 = stn_affine_inference.grid
    dvf_1 = dvf_1.permute(0, 4, 1, 2, 3)

    #Deformable model inference

    dvf_2 = deformable_model_inference(X, coarsely_warped_image) 
    # print("deformable field dvf 2 shape: {}".format(dvf_2.shape))
    # print("========== ============== =============")
    # print()

    # print(" interim warp ")
    interim_warp_field = stn_aggregation_inference(dvf_1,dvf_2)
    # print("interim warp field shape: {}".format(interim_warp_field.shape))
    # print("========== ============== =============")
    # print()

  
    dvf_final = dvf_2 + interim_warp_field
    # print("final dvf shape: {}".format(dvf_final.shape))
    # print("========== ============== =============")
    # print()

    fully_warped_image =  stn_fully_warped_inference(Y, dvf_final)  #final fully warped image , i.e. equation 4 of the image
    # print("final warped image shape: {}".format(fully_warped_image.shape))
    # print("========== ============== =============")
    # print()

    full_warped_tensor = fully_warped_image.detach().to("cpu")
    coarsely_warped_tnsr = coarsely_warped_image.detach().to("cpu")
    #dvf_1_warped_tensor = dvf_1.detach().to("cpu")
    full_warped_np = full_warped_tensor.numpy()
    coarsely_warped_np = coarsely_warped_tnsr.numpy()
    #dvf_1_warped_np = dvf_1_warped_tensor.numpy()

    full_warped_nb = nb.Nifti1Image(full_warped_np[0,0,:,:,:], np.eye(4)) 
    coarsely_warped_nb = nb.Nifti1Image(coarsely_warped_np[0,0,:,:,:], np.eye(4))
    nb.save(full_warped_nb, '/content/drive/My Drive/Image_Registration_Project/dataset_ants_resampled/warped_images/' +  'full_warped_nb_' + str(counter) + '.nii.gz')
    nb.save(coarsely_warped_nb, '/content/drive/My Drive/Image_Registration_Project/dataset_ants_resampled/warped_images/' +  'coarsely_warped_nb_' + str(counter) + '.nii.gz')
    #dvf_1_warped_nb = nb.Nifti1Image(dvf_1_warped_np[0,:,:,:,:], np.eye(4)) 
    #nb.save(dvf_1_warped_nb, '/content/drive/My Drive/Image_Registration_Project/dataset_ants_resampled/' +  'dvf_1_warped_nb_' + str(counter) + '.nii.gz')

    counter = counter + 1

    if(counter > 3):
      break;

fullmodel_inference_loop()

fully_warped_img = nb.load("/content/drive/My Drive/Image_Registration_Project/dataset_ants_resampled/warped_images/full_warped_nb_0.nii.gz")
coarsely_warped_img = nb.load("/content/drive/My Drive/Image_Registration_Project/dataset_ants_resampled/warped_images/coarsely_warped_nb_0.nii.gz")
warped_img_np = fully_warped_img.dataobj
coarsely_warped_img_np = coarsely_warped_img.dataobj
mynb_np1 = load_4D(file_names_t1[0])
mynb_np1 = mynb_np1[0,:,:,:]
mynb_np2 = load_4D(file_names_t1[4])
mynb_np2 = mynb_np2[0,:,:,:]
vol_shape = (128, 128, 128)

mynb_np2.shape

mid_slices_fixed = [np.take(mynb_np1, 55, axis=d) for d in range(3)]
mid_slices_fixed[1] = np.rot90(mid_slices_fixed[1], 1)
mid_slices_fixed[2] = np.rot90(mid_slices_fixed[2], -1)

mid_slices_moving = [np.take(mynb_np2, 55, axis=d) for d in range(3)]
mid_slices_moving[1] = np.rot90(mid_slices_moving[1], 1)
mid_slices_moving[2] = np.rot90(mid_slices_moving[2], -1)

mid_slices_pred = [np.take(warped_img_np, 55, axis=d) for d in range(3)]
mid_slices_pred[1] = np.rot90(mid_slices_pred[1], 1)
mid_slices_pred[2] = np.rot90(mid_slices_pred[2], -1)
ne.plot.slices(mid_slices_fixed + mid_slices_moving + mid_slices_pred, cmaps=['gray'], do_colorbars=True, grid=[3,3]);



plt.imshow(warped_img_np[77, :, :])

plt.imshow(coarsely_warped_img_np[50, :, :])

plt.imshow(mynb_np1[0, 77, :, :])

plt.imshow(mynb_np2[0, 77, :, :])

"""# Miscellaneous

=========== Below are methods for testing outside function =======
"""

dim_prod = 128*128*128
num_bins = 10
bin_centers = np.linspace(0, 0.7, num_bins*2+1)[1::2]
sigma = np.mean(np.diff(bin_centers)) * 0.5
preterm = torch.tensor(1 / (2 * np.square(sigma)))
nb_voxels = torch.tensor([dim_prod], dtype=torch.float32)
print(bin_centers)
print(sigma)
print(preterm)
print(nb_voxels)

y_true = torch.ones(size= (1, 1, 128, 128, 128) )
y_pred = torch.ones(size = (1, 1, 128, 128, 128) )

y_true = torch.randn(size= (1, 1, 128, 128) )
y_pred = torch.randn(size = (1, 1, 128, 128) )

print(mutual_information(y_true, y_pred, sigma, preterm, bin_centers, nb_voxels, dim_prod = 128*128*128))



from sklearn.metrics import mutual_info_score, normalized_mutual_info_score

x1 = y_pred.numpy()
x1 = x1[0, 0, :, :]
x1 = x1.flatten()
x2 = y_true.numpy()
x2 = x2[0, 0, :, :]
x2 = x2.flatten()

def calc_MI(x, y, bins):
    #c_xy = np.histogram2d(x, y, bins)[0]
    mi = normalized_mutual_info_score(x, y)
    return mi

print(calc_MI( np.random.randn((128*128)), np.zeros((128*128,), dtype=float) , bins=10))

torch.prod( torch.tensor(y_true.shape[1:]) ).numpy()

y_true = torch.reshape(y_true, (y_true.shape[0], 2097152, 1) )
y_pred = torch.reshape(y_pred, (y_pred.shape[0], 2097152, 1) )

y_true.shape



vbc = torch.tensor(bin_centers)
vbc = vbc.unsqueeze(0)
vbc = vbc.unsqueeze(0)
vbc.shape

# compute image terms
I_a = torch.exp(- preterm * torch.square(y_true  - vbc))
I_a = I_a/torch.sum(I_a)

I_b = torch.exp(- preterm * torch.square(y_pred  - vbc))
I_b = I_b/torch.sum(I_b)

print(I_a)
print(I_b)

# compute probabilities
I_a_permute = I_a.permute(0,2,1)
pab = torch.matmul(I_a_permute, I_b)  # should be the right size now, nb_labels x nb_bins
pab = pab/nb_voxels
pa = torch.mean(I_a, 1, keepdims=True)
pb = torch.mean(I_b, 1, keepdims=True)

pa

pab

papb = torch.matmul(pa.permute(0,2,1), pb) + torch.finfo(torch.float32).eps
papb

torch.finfo(torch.float32).eps

mi = torch.sum(torch.sum(pab * torch.log(pab/papb + torch.finfo(torch.float32).eps), 1), 1)

mi